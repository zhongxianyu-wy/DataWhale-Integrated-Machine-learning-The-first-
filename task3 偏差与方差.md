# 集成机器学习 task3
## 优化的目的
在训练集中，我们常常以预测值与真实值之间的差距来衡量模型的好坏，但并非是预测值与真实值之间的差距越小越好，更重要的是在测试集中表现情况
## 训练均方误差与测试均方误差
比起训练误差来说，我们更希望测试误差更下，但存在以下几种限制   
1）测试集的真实值我们是未知的    
2）训练集的误差在一定程度上能代表测试集的误差，但是存在训练集误差越低时，反而导致测试集误差越高-----------过拟合   
## 偏差与方差
测试集的误差可以分解为模型的稳定性以及模型预测值与真实值的偏移性以及模型本身无法改变的误差三个部分      
1）模型的稳定性：方差，即代表模型的对样本拟合程度的稳定性，如果模型复杂度越高，则其稳定性越差，其模型方差也越差   
2）预测值与真实值的偏移程度：偏差，代表模型对抽取的部分样本预测值与真实值之间的差距      
一个模型的好坏则应该在上诉两种因素中权衡，一般而言，增加模型的复杂度，会增加模型的方差，但是会减少模型的偏差     
## 测试集误差的估计-----终目的
### 法一 限制模型的复杂度的情况下尽可能拟合训练集
对模型特征加上一个惩罚函数，限制模型的特征数目
### 法二 交叉验证
交叉验证能够做出对训练误差的直接估计，k-折交叉验证同时考虑了模型在不同测试集之间表现的稳定性----即能一定程度上体现方差  
一般交叉验证分为三种方式    
1）按照一定比例划分为训练集与测试集，一般为73、82   
2）k-折交叉验证，将样本划分为K份，每次取一份作为测试集，遍历所有样本   
3）留一法：一般适合小样本量   
## 特征选择/特征消减的方法  
1）向前逐步选择-------贪心算法   
每次增加一个特征，选出当前特征数最优模型的组合，在此基础上进一步增加特征，选出最佳组合，在整个过程中衡量每种特征数最佳组合的测试误差，找到最优组合  
2）最佳子集选择
与上面区别则是，每次增加一个变量都是看所有组合的最佳组合，其计算量大，但能避免贪心算法的局部最优解缺点  
3）L1、L2正则化  
L1和L2正则化都是对线性模型进行特征约束，其中L1是二范数的结果，因此惩罚力度更大，更易造成稀疏矩阵的形成，而L2正则化则是一范数的结果，惩罚力度较小，对异常特征值没有L1敏感   
弹性网络则是融合L1与L2  

## 降维的思想
1）PCA 主成分分析
通过最大投影方差 将原始空间进行重构
将高维特征降维为多个互不相关的特征以最大代表方差程度

## 对模型超参数进行调优
主要包括以上几种方法：   
1）向前逐步优化法----贪心算法     
按照经验对超参数进行重要性排序，先对重要的参数继续评估，选择其他条件不变的情况下，超参数的最佳值，依次往前继续寻找其他最佳的超参数  
2）网格搜索GridSearchCV()--------算力优化：随机搜索网格    
对所有超参数进行网格式排列组合，然后每个组合均进行构建模型，找到最优的一个组合   
3）贝叶斯调参  
给定优化的目标函数(广义的函数，只需指定输入和输出即可，无需知道内部结构以及数学性质)，通过不断地添加样本点来更新目标函数的后验分布(高斯过程,直到后验分布基本贴合于真实分布）。简单的说，就是考虑了上一次参数的信息，从而更好的调整当前的参数。    
A 定义优化函数(rf_cv）   
B 建立模型   
C 定义待优化的参数   
D 得到优化结果，并返回要优化的分数指标   

