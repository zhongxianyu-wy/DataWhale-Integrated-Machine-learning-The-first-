# day2 回归相关知识和推导
## 当遇见一个回归类问题时，以下为分析的步骤：    
1.根据预测目标值的类型确定预测任务为回归/分类（回归为连续型变量）   
2.整理与目标值相关的特征   
3.选择合适的回归指标：主要包括以下四种    
1）MSE：均方误差，即代表预测值与真实值的差的二范数之和，其对异常值更加敏感    
2）MAE：平均绝对误差，即代表预测值与真实值的差的绝对数之和，其对异常值的敏感度不如MSE    
3）R2决定系数：1-残差平方和/总离差平方和 残差平方和越小，越拟合，排除了离差的影响    
4）解释方差得分：1-真实值与预测值之差的方差与真实值方差之比  解释方差的变化，越小则越稳定     
4.选择合适的模型构建，并划分训练集和测试集，优化模型   
5.评估模型并调参至最佳    
## 线性回归模型
目前最常用的为最小二乘法估计：即真实值与预测值之间的二范数之和
 w ^T = (X^TX)^{-1}X^Ty   
其实现可有torch库中的函数torch.lstsq(y, X)实现   
线性回归的话可用sklearn库中的liner_model实现  
```from sklearn import linear_model```                
```lin_reg = linear_model.LinearRegression()```     
```lin_reg.fit(X,y)``` 训练线性回归

## 线性回归的推广
将线性回归推广到非线性回归  
1）多项式回归：引入多项式回归可实现非线性回归  
2）GAM（广义可加模型）：将普通的线性回归的x以复函数的进行替换，比如：   
y=w1x1+w2x2+b：将x1替代为x1=log2x1,x2=e{-x2}
将线性回归转化为非线性组合
PS：可通过GAM的可加性特色，将特征之间的交互作用加上去，如x1*x2   
## LASSO 回归
通过构造一个惩罚函数得到一个较为精炼的模型，使得它压缩一些回归系数，L1正则化系数，其为系数的平方和，与MSE类似，其对异常值的更加敏感，从而导致损失函数在对系数进行约束时，力度更大，从而构建出稀疏矩阵，以排除掉许多无用/或对结果用处较小的特征，因此，其也常常作为特征筛选的一种方法  
## Rigde 回归
与LASSO回归属于同一家族，但其损失函数的附加系数L2正则化则类似与MAE，其对系数的约束能力没有LASSO强，但亦能实现将系数约束到一定范围，减少异常值的出现，亦使得回归变得平滑，从而减少过拟合的程度
## 弹性网络
其处于LASSO和Rigde回归之间，具有部分LASSO稀疏化的特色也具备Rigde保留特征的特色

##回归树  

基于树的回归方法，其实通过特征对结果进行划区分区处理，最后预测值通过区域内的总数或者平均数来代表。回归树的内部节点均表现为一个特征，其根据特征进行下一步的划分，而叶节点则代表从特征划分而来的结果。
优化目的为找到最佳的J个分裂特征以及s区分点，以保证划分区域每个真实值与所代表区域的值的差值最小化
其优点包括：
1）可解释性强，可以用图形可视化表示  
2）可以用定性特征   
3）对缺失值和异常值友好，而异常值则会对普通线性回归造成极大的影响  
4）可以结合树的深度，树的剪枝进行优化  
5）集成随机森林、梯度提升树会有更好的预测结果  
##支持向量机回归（SVR） 
落在f(x)邻域空间中的样本点不需要计算损失，即预测正确的，其余的落在 邻域空间以外的样本需要计算损失。  
简单来讲即将领域空间点与f(WX)距离大于𝜖 的部分要最大化  




